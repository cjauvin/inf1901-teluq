<!doctype html><html lang=fr dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  Les modèles de langage (petits et grands)
  #

Ce module présente l&rsquo;évolution des modèles de langage, des approches classiques
aux modèles modernes fondés sur les transformers. Il montre comment ces systèmes
ont permis de passer de la simple prévision statistique des mots à des
conversations complexes avec les machines.

  Introduction aux modèles de langage
  #

Un modèle de langage est un système qui attribue une probabilité à une
séquence de mots. Son objectif principal est de prévoir le mot suivant dans un
texte, ce qui est essentiel pour des applications comme la reconnaissance
vocale, la traduction automatique ou la recherche d’information."><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://cjauvin.github.io/inf1901-teluq/docs/module4/02-grands-mod%C3%A8les-de-langage/"><meta property="og:site_name" content="INF1901 - Initiation à l'IA : concepts et réflexions"><meta property="og:title" content="Grands modèles de langage"><meta property="og:description" content="Les modèles de langage (petits et grands) # Ce module présente l’évolution des modèles de langage, des approches classiques aux modèles modernes fondés sur les transformers. Il montre comment ces systèmes ont permis de passer de la simple prévision statistique des mots à des conversations complexes avec les machines.
Introduction aux modèles de langage # Un modèle de langage est un système qui attribue une probabilité à une séquence de mots. Son objectif principal est de prévoir le mot suivant dans un texte, ce qui est essentiel pour des applications comme la reconnaissance vocale, la traduction automatique ou la recherche d’information."><meta property="og:locale" content="fr"><meta property="og:type" content="article"><meta property="article:section" content="docs"><title>Grands modèles de langage | INF1901 - Initiation à l'IA : concepts et réflexions</title><link rel=icon href=https://cjauvin.github.io/inf1901-teluq/favicon.png><link rel=manifest href=https://cjauvin.github.io/inf1901-teluq/manifest.json><link rel=canonical href=https://cjauvin.github.io/inf1901-teluq/docs/module4/02-grands-mod%C3%A8les-de-langage/><link rel=stylesheet href=https://cjauvin.github.io/inf1901-teluq/book.min.c2a3a3930cc3c92484d4c7886a609454c1ccc7fbe839b6904dde85b081e514b4.css integrity="sha256-wqOjkwzDySSE1MeIamCUVMHMx/voObaQTd6FsIHlFLQ=" crossorigin=anonymous><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=https://cjauvin.github.io/inf1901-teluq/><span>INF1901 - Initiation à l'IA : concepts et réflexions</span></a></h2><ul><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/pr%C3%A9sentation/>Présentation du cours</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/philosophie/>Philosophie du cours</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/livres/>Les livres</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/travaux-not%C3%A9s/>Travaux notés</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/google-sheets/>Google Sheets</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/assistants-intelligents/>Usage de l'IA</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/ressources-additionnelles/>Ressources supplémentaires</a></li><li><input type=checkbox id=section-8907ef67e4ecdf3db52171242f091373 class=toggle>
<label for=section-8907ef67e4ecdf3db52171242f091373 class=flex><a href=https://cjauvin.github.io/inf1901-teluq/docs/module1/ class=flex-auto>Module 1 - Intelligence artificielle</a></label><ul><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module1/activit%C3%A9s/>Activités</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module1/travail-not%C3%A9/>Travail noté 1</a></li></ul></li><li><input type=checkbox id=section-0e47afea11237f1612150a31e7123755 class=toggle>
<label for=section-0e47afea11237f1612150a31e7123755 class=flex><a href=https://cjauvin.github.io/inf1901-teluq/docs/module2/ class=flex-auto>Module 2 - Apprentissage automatique</a></label><ul><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module2/01-sc%C3%A9nario-r%C3%A9el/>Un scénario imaginé</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module2/02-diff%C3%A9rence-avec-la-prog/>AA versus programmation</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module2/03-diff%C3%A9rence-avec-lia/>AA versus IA</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module2/04-ia-versus-stats/>AA versus statistiques</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module2/05-repr%C3%A9sentation-des-donn%C3%A9es/>Représentation des données</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module2/06-paradigmes/>Paradigmes de l'AA</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module2/07-applications/>Applications de l'AA</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module2/08-programmation/>AA et programmation</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module2/travail-not%C3%A9-2/>Travail noté 2</a></li></ul></li><li><input type=checkbox id=section-9cc60359ac12343378e5fa944b3863f7 class=toggle>
<label for=section-9cc60359ac12343378e5fa944b3863f7 class=flex><a href=https://cjauvin.github.io/inf1901-teluq/docs/module3/ class=flex-auto>Module 3 - Réseaux de neurones et apprentissage profond</a></label><ul><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module3/01-r%C3%A9seaux-de-neurones/>Les réseaux de neurones</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module3/02-3blue1brown/>3Blue1Brown</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module3/travail-not%C3%A9-3/>Travail noté 3</a></li></ul></li><li><input type=checkbox id=section-d23b9c643b16319dcdaeed5376bcec9c class=toggle checked>
<label for=section-d23b9c643b16319dcdaeed5376bcec9c class=flex><a href=https://cjauvin.github.io/inf1901-teluq/docs/module4/ class=flex-auto>Module 4 - IA générative et grands modèles de langage</a></label><ul><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module4/01-ia-g%C3%A9n%C3%A9rative/>IA générative</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module4/02-grands-mod%C3%A8les-de-langage/ class=active>Grands modèles de langage</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module4/03-3blue1brown/>3Blue1Brown</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module4/travail-not%C3%A9-4/>Travail noté 4</a></li></ul></li><li><input type=checkbox id=section-b90ca6a3362bb22be66d4f5e1f8ab16c class=toggle>
<label for=section-b90ca6a3362bb22be66d4f5e1f8ab16c class=flex><a href=https://cjauvin.github.io/inf1901-teluq/docs/module5/ class=flex-auto>Module 5 - Enjeux éthiques et philosophiques</a></label><ul><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module5/conversation/>Conversation autour de l'IA</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module5/travail-not%C3%A9-5/>Travail noté 5</a></li></ul></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/conclusion/>Conclusion</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=https://cjauvin.github.io/inf1901-teluq/svg/menu.svg class=book-icon alt=Menu></label><h3>Grands modèles de langage</h3><label for=toc-control><img src=https://cjauvin.github.io/inf1901-teluq/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#introduction-aux-modèles-de-langage>Introduction aux modèles de langage</a></li><li><a href=#premiers-réseaux-de-neurones-appliqués-au-langage-et-représentations-vectorielles>Premiers réseaux de neurones appliqués au langage et représentations vectorielles</a></li><li><a href=#réseaux-neuronaux-récurrents-et-modèles-séquence-à-séquence>Réseaux neuronaux récurrents et modèles séquence-à-séquence</a></li><li><a href=#lavènement-des-transformers>L’avènement des Transformers</a><ul><li><a href=#le-principe-de-lauto-attention>Le principe de l’auto-attention</a></li><li><a href=#les-principaux-composants-dun-transformer>Les principaux composants d’un Transformer</a></li><li><a href=#les-embeddings-et-la-position>Les embeddings et la position</a></li><li><a href=#traitement-parallèle-et-performance>Traitement parallèle et performance</a></li></ul></li><li><a href=#des-modèles-prédictifs-aux-assistants-conversationnels>Des modèles prédictifs aux assistants conversationnels</a><ul><li><a href=#les-modèles-instructgpt-et-chatgpt>Les modèles InstructGPT et ChatGPT</a></li><li><a href=#lapprentissage-par-renforcement-avec-retour-humain-rlhf>L’apprentissage par renforcement avec retour humain (RLHF)</a></li></ul></li><li><a href=#conclusion>Conclusion</a></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=les-modèles-de-langage-petits-et-grands>Les modèles de langage (petits et grands)
<a class=anchor href=#les-mod%c3%a8les-de-langage-petits-et-grands>#</a></h1><p>Ce module présente l&rsquo;évolution des modèles de langage, des approches classiques
aux modèles modernes fondés sur les transformers. Il montre comment ces systèmes
ont permis de passer de la simple prévision statistique des mots à des
conversations complexes avec les machines.</p><h2 id=introduction-aux-modèles-de-langage>Introduction aux modèles de langage
<a class=anchor href=#introduction-aux-mod%c3%a8les-de-langage>#</a></h2><p>Un <em>modèle de langage</em> est un système qui attribue une probabilité à une
séquence de mots. Son objectif principal est de prévoir le mot suivant dans un
texte, ce qui est essentiel pour des applications comme la reconnaissance
vocale, la traduction automatique ou la recherche d’information.</p><p>Les approches classiques utilisaient surtout des modèles statistiques appelés
<em>n-grammes</em>. Dans un modèle n-gramme, la probabilité d’un mot dépend des (n-1)
mots précédents. Par exemple, un trigramme (n=3) prend en compte les deux mots
qui précèdent chaque mot à prédire. Ces modèles avaient deux limites majeures :</p><ul><li>Ils nécessitaient des corpus très volumineux pour estimer les probabilités.</li><li>Ils ne représentaient pas les liens de sens entre les mots (par exemple,
« chien » et « chiens » étaient traités comme totalement distincts).</li></ul><h2 id=premiers-réseaux-de-neurones-appliqués-au-langage-et-représentations-vectorielles>Premiers réseaux de neurones appliqués au langage et représentations vectorielles
<a class=anchor href=#premiers-r%c3%a9seaux-de-neurones-appliqu%c3%a9s-au-langage-et-repr%c3%a9sentations-vectorielles>#</a></h2><p>Avec l’essor de l’apprentissage machine et des réseaux neuronaux, des chercheurs
ont cherché à dépasser ces limites. Passer d’un modèle purement statistique à un
modèle <em>neuronal</em> a permis de représenter les mots sous une forme plus compacte
et plus riche en information.</p><p>C’est dans ce contexte qu’ont été introduites les <em>représentations vectorielles
des mots</em>, connues sous le nom de <em>word embeddings</em>. Le modèle Word2Vec, proposé
par Mikolov et ses collègues en 2013, a permis d’apprendre un vecteur dense (par
exemple, de 300 dimensions) pour chaque mot. Ces vecteurs capturent les
relations sémantiques et contextuelles. On pouvait ainsi observer des analogies
célèbres :</p><pre tabindex=0><code>  roi - homme + femme ≈ reine
</code></pre><p>Ces représentations ont permis de mieux généraliser aux mots rares ou nouveaux
et ont constitué une avancée majeure.</p><h2 id=réseaux-neuronaux-récurrents-et-modèles-séquence-à-séquence>Réseaux neuronaux récurrents et modèles séquence-à-séquence
<a class=anchor href=#r%c3%a9seaux-neuronaux-r%c3%a9currents-et-mod%c3%a8les-s%c3%a9quence-%c3%a0-s%c3%a9quence>#</a></h2><p>Pour dépasser la rigidité des n-grammes, les <em>réseaux neuronaux récurrents</em>
(RNN) ont été introduits. Un RNN traite la séquence mot par mot, en maintenant
une mémoire interne qui encode le contexte précédent. Cela permet, en théorie,
de prendre en compte des dépendances longues.</p><p>Cependant, les RNN simples présentaient des difficultés :</p><ul><li>Les gradients disparaissaient au fil des étapes (<em>problème du gradient qui s’annule</em>), rendant difficile l’apprentissage des relations à long terme.</li><li>L’entraînement était lent, car les séquences devaient être traitées une étape après l’autre.</li></ul><p>Pour résoudre ces problèmes, les architectures <em>LSTM</em> (Long Short-Term Memory)
et <em>GRU</em> (Gated Recurrent Unit) ont été développées. Elles utilisent des
mécanismes appelés <em>portes</em> qui régulent l’information qui circule dans la
mémoire.</p><p>Ces modèles ont permis de créer les architectures <em>séquence-à-séquence</em>
(Seq2Seq), capables de transformer une séquence en une autre (par exemple,
traduire une phrase d’une langue à une autre).</p><h2 id=lavènement-des-transformers>L’avènement des Transformers
<a class=anchor href=#lav%c3%a8nement-des-transformers>#</a></h2><p>En 2017, Vaswani et ses collègues publient l’article « Attention Is All You
Need », qui marque une rupture profonde avec les approches précédentes. Le
<em>Transformer</em> abandonne la récurrence et repose uniquement sur un mécanisme
d’<em>auto-attention</em>.</p><h3 id=le-principe-de-lauto-attention>Le principe de l’auto-attention
<a class=anchor href=#le-principe-de-lauto-attention>#</a></h3><p>L’auto-attention permet à chaque mot de la séquence de tenir compte de tous les
autres mots en parallèle. Concrètement :</p><ul><li>Chaque mot est transformé en trois vecteurs : <em>requête</em> (Query), <em>clé</em> (Key) et <em>valeur</em> (Value).</li><li>Pour un mot donné, on compare sa requête avec toutes les clés des autres mots, ce qui produit des scores d’attention.</li><li>Ces scores sont normalisés avec une fonction softmax, puis servent à pondérer la combinaison des valeurs.</li></ul><p>Ainsi, chaque mot obtient une représentation enrichie du contexte global, sans
avoir besoin de parcourir la séquence dans l’ordre.</p><h3 id=les-principaux-composants-dun-transformer>Les principaux composants d’un Transformer
<a class=anchor href=#les-principaux-composants-dun-transformer>#</a></h3><p>Un bloc Transformer comprend plusieurs éléments :</p><ul><li>Un mécanisme d’<em>auto-attention multi-têtes</em> (chaque tête apprend à repérer différents types de relations).</li><li>Un réseau de neurones <em>feedforward</em> appliqué indépendamment à chaque position.</li><li>Des normalisations de couche (<em>Layer Normalization</em>) et des connexions résiduelles qui stabilisent et accélèrent l’apprentissage.</li></ul><h3 id=les-embeddings-et-la-position>Les embeddings et la position
<a class=anchor href=#les-embeddings-et-la-position>#</a></h3><p>Contrairement aux RNN, le Transformer ne possède pas naturellement la notion
d’ordre des mots. Il faut donc ajouter des <em>embeddings positionnels</em> qui donnent
à chaque mot une information sur sa place dans la séquence. Ces embeddings
peuvent être calculés avec des fonctions sinusoïdales ou appris directement
pendant l’entraînement.</p><h3 id=traitement-parallèle-et-performance>Traitement parallèle et performance
<a class=anchor href=#traitement-parall%c3%a8le-et-performance>#</a></h3><p>L’un des avantages majeurs du Transformer est que toutes les positions peuvent
être traitées en même temps (<em>parallélisme</em>). Cette caractéristique rend
l’entraînement beaucoup plus rapide et plus efficace que celui des réseaux
récurrents.</p><p>Ces propriétés ont permis de concevoir des modèles très puissants, comme BERT,
GPT, T5 et leurs successeurs.</p><h2 id=des-modèles-prédictifs-aux-assistants-conversationnels>Des modèles prédictifs aux assistants conversationnels
<a class=anchor href=#des-mod%c3%a8les-pr%c3%a9dictifs-aux-assistants-conversationnels>#</a></h2><p>Les premiers Transformers servaient surtout à prédire le mot suivant ou à
compléter un texte. Pour passer à des modèles capables de répondre à des
instructions et de dialoguer avec les humains, il a fallu plusieurs étapes
supplémentaires.</p><h3 id=les-modèles-instructgpt-et-chatgpt>Les modèles InstructGPT et ChatGPT
<a class=anchor href=#les-mod%c3%a8les-instructgpt-et-chatgpt>#</a></h3><p>Ces modèles sont obtenus par un <em>ajustement fin</em> (fine-tuning) des Transformers
de base sur des exemples de conversations et des instructions annotées par des
humains. Ce processus permet d’orienter le comportement du modèle vers des
réponses plus pertinentes et utiles.</p><h3 id=lapprentissage-par-renforcement-avec-retour-humain-rlhf>L’apprentissage par renforcement avec retour humain (RLHF)
<a class=anchor href=#lapprentissage-par-renforcement-avec-retour-humain-rlhf>#</a></h3><p>Le <em>Reinforcement Learning from Human Feedback</em> (apprentissage par renforcement avec retour humain) se déroule en plusieurs étapes :</p><ol><li>Le modèle produit différentes réponses à une même question.</li><li>Des évaluateurs humains les classent de la meilleure à la moins bonne.</li><li>Ces classements servent à entraîner un <em>modèle de récompense</em> qui estime la qualité des réponses.</li><li>Le modèle principal est ensuite optimisé par renforcement (souvent avec un algorithme appelé PPO, Proximal Policy Optimization) pour maximiser cette récompense.</li></ol><p>Ce processus permet d’aligner les modèles avec des critères humains de politesse, de sécurité et de pertinence.</p><h2 id=conclusion>Conclusion
<a class=anchor href=#conclusion>#</a></h2><p>Les modèles de langage ont évolué d’outils statistiques simples à des systèmes
capables de mener des conversations détaillées et nuancées. Comprendre leur
histoire permet de mesurer à la fois leur puissance et les défis qu’ils posent.</p></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><div class="flex flex-wrap justify-between"><a href=https://cjauvin.github.io/inf1901-teluq/docs/module4/01-ia-g%C3%A9n%C3%A9rative/ class="flex align-center float-left book-icon"><img src=https://cjauvin.github.io/inf1901-teluq/svg/backward.svg alt=Previous title="IA générative">
<span>IA générative</span>
</a><a href=https://cjauvin.github.io/inf1901-teluq/docs/module4/03-3blue1brown/ class="flex align-center float-right book-icon"><span>3Blue1Brown</span>
<img src=https://cjauvin.github.io/inf1901-teluq/svg/forward.svg alt=Next title=3Blue1Brown></a></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#introduction-aux-modèles-de-langage>Introduction aux modèles de langage</a></li><li><a href=#premiers-réseaux-de-neurones-appliqués-au-langage-et-représentations-vectorielles>Premiers réseaux de neurones appliqués au langage et représentations vectorielles</a></li><li><a href=#réseaux-neuronaux-récurrents-et-modèles-séquence-à-séquence>Réseaux neuronaux récurrents et modèles séquence-à-séquence</a></li><li><a href=#lavènement-des-transformers>L’avènement des Transformers</a><ul><li><a href=#le-principe-de-lauto-attention>Le principe de l’auto-attention</a></li><li><a href=#les-principaux-composants-dun-transformer>Les principaux composants d’un Transformer</a></li><li><a href=#les-embeddings-et-la-position>Les embeddings et la position</a></li><li><a href=#traitement-parallèle-et-performance>Traitement parallèle et performance</a></li></ul></li><li><a href=#des-modèles-prédictifs-aux-assistants-conversationnels>Des modèles prédictifs aux assistants conversationnels</a><ul><li><a href=#les-modèles-instructgpt-et-chatgpt>Les modèles InstructGPT et ChatGPT</a></li><li><a href=#lapprentissage-par-renforcement-avec-retour-humain-rlhf>L’apprentissage par renforcement avec retour humain (RLHF)</a></li></ul></li><li><a href=#conclusion>Conclusion</a></li></ul></nav></div></aside></main></body></html>