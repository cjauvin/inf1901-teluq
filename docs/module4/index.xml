<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Module 4 - IA générative et grands modèles de langage on INF1901 - Initiation à l'IA : concepts et réflexions</title><link>https://cjauvin.github.io/inf1901-teluq/docs/module4/</link><description>Recent content in Module 4 - IA générative et grands modèles de langage on INF1901 - Initiation à l'IA : concepts et réflexions</description><generator>Hugo</generator><language>fr</language><atom:link href="https://cjauvin.github.io/inf1901-teluq/docs/module4/index.xml" rel="self" type="application/rss+xml"/><item><title>IA générative</title><link>https://cjauvin.github.io/inf1901-teluq/docs/module4/01-ia-g%C3%A9n%C3%A9rative/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://cjauvin.github.io/inf1901-teluq/docs/module4/01-ia-g%C3%A9n%C3%A9rative/</guid><description>&lt;h1 id="lia-générative"&gt;
 L&amp;rsquo;IA générative
 &lt;a class="anchor" href="#lia-g%c3%a9n%c3%a9rative"&gt;#&lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;L’IA générative désigne une catégorie d’intelligences artificielles capables de créer de nouveaux contenus à partir de données existantes. Ces systèmes, souvent basés sur des réseaux de neurones profonds, apprennent à imiter, transformer ou inventer des images, des vidéos, du texte ou de la musique.&lt;/p&gt;
&lt;p&gt;Les premiers modèles de génération de texte utilisaient des approches statistiques simples, comme les n-grammes, qui prédisaient le mot suivant en fonction des précédents. L’arrivée des réseaux de neurones, puis des architectures avancées comme les Transformers, a révolutionné le domaine. Les modèles modernes, tels que GPT, sont capables de comprendre des instructions complexes, de dialoguer, de traduire et de générer des textes longs et structurés.&lt;/p&gt;</description></item><item><title>Grands modèles de langage</title><link>https://cjauvin.github.io/inf1901-teluq/docs/module4/02-grands-mod%C3%A8les-de-langage/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://cjauvin.github.io/inf1901-teluq/docs/module4/02-grands-mod%C3%A8les-de-langage/</guid><description>&lt;h1 id="les-modèles-de-langage-petits-et-grands"&gt;
 Les modèles de langage (petits et grands)
 &lt;a class="anchor" href="#les-mod%c3%a8les-de-langage-petits-et-grands"&gt;#&lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;Ce module présente l&amp;rsquo;évolution des modèles de langage, des approches classiques
aux modèles modernes fondés sur les transformers. Il montre comment ces systèmes
ont permis de passer de la simple prévision statistique des mots à des
conversations complexes avec les machines.&lt;/p&gt;
&lt;h2 id="introduction-aux-modèles-de-langage"&gt;
 Introduction aux modèles de langage
 &lt;a class="anchor" href="#introduction-aux-mod%c3%a8les-de-langage"&gt;#&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Un &lt;em&gt;modèle de langage&lt;/em&gt; est un système qui attribue une probabilité à une
séquence de mots. Son objectif principal est de prévoir le mot suivant dans un
texte, ce qui est essentiel pour des applications comme la reconnaissance
vocale, la traduction automatique ou la recherche d’information.&lt;/p&gt;</description></item><item><title>3Blue1Brown</title><link>https://cjauvin.github.io/inf1901-teluq/docs/module4/03-3blue1brown/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://cjauvin.github.io/inf1901-teluq/docs/module4/03-3blue1brown/</guid><description>&lt;h1 id="vidéos-de-3blue1brown"&gt;
 Vidéos de 3Blue1Brown
 &lt;a class="anchor" href="#vid%c3%a9os-de-3blue1brown"&gt;#&lt;/a&gt;
&lt;/h1&gt;
&lt;h2 id="introduction-aux-modèles-de-langages"&gt;
 Introduction aux modèles de langages
 &lt;a class="anchor" href="#introduction-aux-mod%c3%a8les-de-langages"&gt;#&lt;/a&gt;
&lt;/h2&gt;
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;"&gt;
 &lt;iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/LPZh9BOjkQs?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video"&gt;&lt;/iframe&gt;
 &lt;/div&gt;

&lt;h2 id="introduction-aux-transformers"&gt;
 Introduction aux transformers
 &lt;a class="anchor" href="#introduction-aux-transformers"&gt;#&lt;/a&gt;
&lt;/h2&gt;
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;"&gt;
 &lt;iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/wjZofJX0v4M?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video"&gt;&lt;/iframe&gt;
 &lt;/div&gt;

&lt;h2 id="introduction-au-mécanisme-dattention"&gt;
 Introduction au mécanisme d&amp;rsquo;attention
 &lt;a class="anchor" href="#introduction-au-m%c3%a9canisme-dattention"&gt;#&lt;/a&gt;
&lt;/h2&gt;
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;"&gt;
 &lt;iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/eMlx5fFNoYc?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video"&gt;&lt;/iframe&gt;
 &lt;/div&gt;

&lt;h2 id="comment-un-gml-peut-storer-linformation"&gt;
 Comment un GML peut storer l&amp;rsquo;information
 &lt;a class="anchor" href="#comment-un-gml-peut-storer-linformation"&gt;#&lt;/a&gt;
&lt;/h2&gt;
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;"&gt;
 &lt;iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/9-Jl0dxWQs8?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video"&gt;&lt;/iframe&gt;
 &lt;/div&gt;</description></item><item><title>Travail noté 4</title><link>https://cjauvin.github.io/inf1901-teluq/docs/module4/travail-not%C3%A9-4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://cjauvin.github.io/inf1901-teluq/docs/module4/travail-not%C3%A9-4/</guid><description>&lt;h1 id="un-mini-chatgpt-dans-google-sheets-travail-noté-4"&gt;
 Un mini ChatGPT dans Google Sheets (travail noté 4)
 &lt;a class="anchor" href="#un-mini-chatgpt-dans-google-sheets-travail-not%c3%a9-4"&gt;#&lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;Un modèle de langage est un outil mathématique qui permet de modéliser la
distribution statistique des mots : en présence (ou dans le contexte) de
certains mots, quel mot a tendance à suivre, et dans quelle proportion des cas
(c&amp;rsquo;est-à-dire avec quelle probabilité). Un modèle de langage n&amp;rsquo;est pas un objet
abstrait qui décrit une réalité théorique : il s&amp;rsquo;agit d&amp;rsquo;un modèle statistique
entraîné sur des données particulières. De la même manière qu&amp;rsquo;un modèle de
prédiction de la température pour la ville de Montréal est différent d&amp;rsquo;un modèle
pour la ville de Québec, un modèle de langage créé par exemple à partir des
données de 100 livres écrits au 19e siècle, et un autre à partir de 100
livres écrits au 20e siècle, seront deux modèles distincts, et auront des
propriétés statistiques très différentes.&lt;/p&gt;</description></item></channel></rss>