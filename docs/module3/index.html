<!doctype html><html lang=fr dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
Module 3 - Réseaux de neurones et apprentissage profond# Ce module explore l’univers fascinant des réseaux de neurones artificiels, moteurs de la révolution moderne de l’apprentissage profond. Pendant plusieurs décennies, ces réseaux étaient perçus comme des outils intéressants en théorie, mais dont l’efficacité réelle restait limitée par la puissance des ordinateurs et la difficulté à les entraîner en profondeur. Tout a changé en 2012, lorsque trois chercheurs de l’Université de Toronto — Alex Krizhevsky, Ilya Sutskever et Geoffrey Hinton — ont mis au point un modèle baptisé AlexNet. Ce réseau de neurones profond, composé de huit couches apprenant des représentations hiérarchiques des images, a été entraîné sur des cartes graphiques (GPU), alors principalement utilisées pour les jeux vidéo. Cette innovation matérielle, combinée à des idées nouvelles comme la fonction d’activation ReLU et la technique de dropout (pour éviter le surapprentissage), a permis à AlexNet d’accomplir un exploit spectaculaire : réduire de moitié le taux d’erreur lors du concours international ImageNet, qui consistait à reconnaître et classifier plus d’un million d’images en mille catégories. Ce résultat, jusque-là inimaginable, a marqué un véritable point de bascule : en quelques mois, toute la communauté scientifique a compris qu’il était possible d’atteindre des performances inégalées grâce aux réseaux de neurones profonds, déclenchant une explosion de recherches, d’applications commerciales et de nouvelles architectures qui allaient bouleverser non seulement la vision par ordinateur, mais aussi le traitement automatique du langage, la reconnaissance vocale et bien d’autres domaines. Ce module vous guidera dans la découverte de ce tournant historique et des concepts fondamentaux qui l’ont rendu possible.
"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://cjauvin.github.io/inf1901-teluq/docs/module3/"><meta property="og:site_name" content="INF1901 - Initiation à l'IA : concepts et réflexions"><meta property="og:title" content="Module 3 - Réseaux de neurones et apprentissage profond"><meta property="og:description" content="Module 3 - Réseaux de neurones et apprentissage profond# Ce module explore l’univers fascinant des réseaux de neurones artificiels, moteurs de la révolution moderne de l’apprentissage profond. Pendant plusieurs décennies, ces réseaux étaient perçus comme des outils intéressants en théorie, mais dont l’efficacité réelle restait limitée par la puissance des ordinateurs et la difficulté à les entraîner en profondeur. Tout a changé en 2012, lorsque trois chercheurs de l’Université de Toronto — Alex Krizhevsky, Ilya Sutskever et Geoffrey Hinton — ont mis au point un modèle baptisé AlexNet. Ce réseau de neurones profond, composé de huit couches apprenant des représentations hiérarchiques des images, a été entraîné sur des cartes graphiques (GPU), alors principalement utilisées pour les jeux vidéo. Cette innovation matérielle, combinée à des idées nouvelles comme la fonction d’activation ReLU et la technique de dropout (pour éviter le surapprentissage), a permis à AlexNet d’accomplir un exploit spectaculaire : réduire de moitié le taux d’erreur lors du concours international ImageNet, qui consistait à reconnaître et classifier plus d’un million d’images en mille catégories. Ce résultat, jusque-là inimaginable, a marqué un véritable point de bascule : en quelques mois, toute la communauté scientifique a compris qu’il était possible d’atteindre des performances inégalées grâce aux réseaux de neurones profonds, déclenchant une explosion de recherches, d’applications commerciales et de nouvelles architectures qui allaient bouleverser non seulement la vision par ordinateur, mais aussi le traitement automatique du langage, la reconnaissance vocale et bien d’autres domaines. Ce module vous guidera dans la découverte de ce tournant historique et des concepts fondamentaux qui l’ont rendu possible."><meta property="og:locale" content="fr"><meta property="og:type" content="website"><meta itemprop=name content="Module 3 - Réseaux de neurones et apprentissage profond"><meta itemprop=description content="Module 3 - Réseaux de neurones et apprentissage profond# Ce module explore l’univers fascinant des réseaux de neurones artificiels, moteurs de la révolution moderne de l’apprentissage profond. Pendant plusieurs décennies, ces réseaux étaient perçus comme des outils intéressants en théorie, mais dont l’efficacité réelle restait limitée par la puissance des ordinateurs et la difficulté à les entraîner en profondeur. Tout a changé en 2012, lorsque trois chercheurs de l’Université de Toronto — Alex Krizhevsky, Ilya Sutskever et Geoffrey Hinton — ont mis au point un modèle baptisé AlexNet. Ce réseau de neurones profond, composé de huit couches apprenant des représentations hiérarchiques des images, a été entraîné sur des cartes graphiques (GPU), alors principalement utilisées pour les jeux vidéo. Cette innovation matérielle, combinée à des idées nouvelles comme la fonction d’activation ReLU et la technique de dropout (pour éviter le surapprentissage), a permis à AlexNet d’accomplir un exploit spectaculaire : réduire de moitié le taux d’erreur lors du concours international ImageNet, qui consistait à reconnaître et classifier plus d’un million d’images en mille catégories. Ce résultat, jusque-là inimaginable, a marqué un véritable point de bascule : en quelques mois, toute la communauté scientifique a compris qu’il était possible d’atteindre des performances inégalées grâce aux réseaux de neurones profonds, déclenchant une explosion de recherches, d’applications commerciales et de nouvelles architectures qui allaient bouleverser non seulement la vision par ordinateur, mais aussi le traitement automatique du langage, la reconnaissance vocale et bien d’autres domaines. Ce module vous guidera dans la découverte de ce tournant historique et des concepts fondamentaux qui l’ont rendu possible."><meta itemprop=wordCount content="382"><title>Module 3 - Réseaux de neurones et apprentissage profond | INF1901 - Initiation à l'IA : concepts et réflexions</title><link rel=icon href=https://cjauvin.github.io/inf1901-teluq/images/paperclip-logo.png><link rel=manifest href=https://cjauvin.github.io/inf1901-teluq/manifest.json><link rel=canonical href=https://cjauvin.github.io/inf1901-teluq/docs/module3/><link rel=stylesheet href=https://cjauvin.github.io/inf1901-teluq/book.min.165c71395eadf4cc8cb595801cc1691b0169871bbe93de1afb5ecb0f5b7c33ae.css integrity="sha256-FlxxOV6t9MyMtZWAHMFpGwFphxu+k94a+17LD1t8M64=" crossorigin=anonymous><link rel=alternate type=application/rss+xml href=https://cjauvin.github.io/inf1901-teluq/docs/module3/index.xml title="INF1901 - Initiation à l'IA : concepts et réflexions"><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=https://cjauvin.github.io/inf1901-teluq/css/applet.css></head><body dir=ltr class="book-kind-section book-type-docs"><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=https://cjauvin.github.io/inf1901-teluq/><img src=https://cjauvin.github.io/inf1901-teluq/images/paperclip-logo.png alt=Logo><span>INF1901 - Initiation à l'IA : concepts et réflexions</span></a></h2><ul><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/pr%C3%A9sentation/>Présentation du cours</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/philosophie/>Approche pédagogique du cours</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/professeurs/>Les professeurs</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/livres/>Les livres</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/travaux-not%C3%A9s/>Travaux notés</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/feuille-de-route/>Feuille de route</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/google-sheets/>Google Sheets</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/assistants-intelligents/>Usage de l'IA</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/changelog/>Évolution du cours (venez voir de temps en temps!)</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/discord/>Serveur Discord</a></li><li><input type=checkbox id=section-8907ef67e4ecdf3db52171242f091373 class=toggle>
<label for=section-8907ef67e4ecdf3db52171242f091373 class=flex><a href=https://cjauvin.github.io/inf1901-teluq/docs/module1/>Module 1 - Intelligence artificielle</a>
<img src=https://cjauvin.github.io/inf1901-teluq/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module1/activit%C3%A9s/>Activités</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module1/travail-not%C3%A9-1/>Travail noté 1</a></li></ul></li><li><input type=checkbox id=section-0e47afea11237f1612150a31e7123755 class=toggle>
<label for=section-0e47afea11237f1612150a31e7123755 class=flex><a href=https://cjauvin.github.io/inf1901-teluq/docs/module2/>Module 2 - Apprentissage automatique</a>
<img src=https://cjauvin.github.io/inf1901-teluq/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module2/sc%C3%A9nario-r%C3%A9el/>Un scénario réaliste pour se faire tout d'abord une idée</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module2/diff%C3%A9rence-avec-x/>AA versus X</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module2/les-donn%C3%A9es/>Que sont les données?</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module2/algo-le-plus-simple/>L'algorithme le plus simple</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module2/similarit%C3%A9/>Le concept de similarité</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module2/mod%C3%A8les/>Qu'est-ce qu'un modèle?</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module2/les-paradigmes/>Les paradigmes de l'AA</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module2/apprentissage-supervis%C3%A9/>Apprentissage supervisé</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module2/apprentissage-non-supervis%C3%A9/>Apprentissage non supervisé</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module2/travail-not%C3%A9-2/>Travail noté 2</a></li></ul></li><li><input type=checkbox id=section-9cc60359ac12343378e5fa944b3863f7 class=toggle checked>
<label for=section-9cc60359ac12343378e5fa944b3863f7 class=flex><a href=https://cjauvin.github.io/inf1901-teluq/docs/module3/ class=active>Module 3 - Réseaux de neurones et apprentissage profond</a>
<img src=https://cjauvin.github.io/inf1901-teluq/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module3/r%C3%A9seaux-de-neurones/>Les réseaux de neurones</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module3/02-3blue1brown/>3Blue1Brown</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module3/architectures-avanc%C3%A9es/>Architectures avancées</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module3/aa-adverse/>Apprentissage automatique adverse</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module3/travail-not%C3%A9-3/>Travail noté 3</a></li></ul></li><li><input type=checkbox id=section-d23b9c643b16319dcdaeed5376bcec9c class=toggle>
<label for=section-d23b9c643b16319dcdaeed5376bcec9c class=flex><a href=https://cjauvin.github.io/inf1901-teluq/docs/module4/>Module 4 - IA générative et grands modèles de langage</a>
<img src=https://cjauvin.github.io/inf1901-teluq/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module4/01-ia-g%C3%A9n%C3%A9rative/>IA générative</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module4/02-grands-mod%C3%A8les-de-langage/>Grands modèles de langage</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module4/03-3blue1brown/>3Blue1Brown</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module4/travail-not%C3%A9-4/>Travail noté 4</a></li></ul></li><li><input type=checkbox id=section-b90ca6a3362bb22be66d4f5e1f8ab16c class=toggle>
<label for=section-b90ca6a3362bb22be66d4f5e1f8ab16c class=flex><a href=https://cjauvin.github.io/inf1901-teluq/docs/module5/>Module 5 - Autour de l'IA</a>
<img src=https://cjauvin.github.io/inf1901-teluq/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module5/attitudes/>Attitudes à l'égard de l'IA</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module5/conversation/>Conversation synoptique autour de l'IA</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module5/travail-not%C3%A9-5/>Travail noté 5</a></li></ul></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/ressources-additionnelles/>Ressources supplémentaires</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/conclusion/>Conclusion</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class="book-header hidden"><div class="flex align-center justify-between"><label for=menu-control><img src=https://cjauvin.github.io/inf1901-teluq/icons/menu.svg class=book-icon alt=Menu></label><h3>Module 3 - Réseaux de neurones et apprentissage profond</h3><label for=toc-control><img src=https://cjauvin.github.io/inf1901-teluq/icons/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class=hidden><nav id=TableOfContents><ul><li><a href=#but-et-objectifs>But et objectifs</a></li><li><a href=#durée>Durée</a></li><li><a href=#évaluation>Évaluation</a></li></ul></nav></aside></header><article class="markdown book-article"><p><img src=https://cjauvin.github.io/inf1901-teluq/images/neural-network.jpg alt></p><h1 id=module-3---réseaux-de-neurones-et-apprentissage-profond>Module 3 - Réseaux de neurones et apprentissage profond<a class=anchor href=#module-3---r%c3%a9seaux-de-neurones-et-apprentissage-profond>#</a></h1><p>Ce module explore l’univers fascinant des réseaux de neurones artificiels,
moteurs de la révolution moderne de l’<strong>apprentissage profond</strong>. Pendant
plusieurs décennies, ces réseaux étaient perçus comme des outils intéressants en
théorie, mais dont l’efficacité réelle restait limitée par la puissance des
ordinateurs et la difficulté à les entraîner en profondeur. Tout a changé en
2012, lorsque trois chercheurs de l’Université de Toronto — Alex Krizhevsky,
Ilya Sutskever et Geoffrey Hinton — ont mis au point un modèle baptisé AlexNet.
Ce réseau de neurones profond, composé de huit couches apprenant des
représentations hiérarchiques des images, a été entraîné sur des cartes
graphiques (GPU), alors principalement utilisées pour les jeux vidéo. Cette
innovation matérielle, combinée à des idées nouvelles comme la fonction
d’activation ReLU et la technique de dropout (pour éviter le surapprentissage),
a permis à AlexNet d’accomplir un exploit spectaculaire : réduire de moitié le
taux d’erreur lors du concours international ImageNet, qui consistait à
reconnaître et classifier plus d’un million d’images en mille catégories. Ce
résultat, jusque-là inimaginable, a marqué un véritable point de bascule : en
quelques mois, toute la communauté scientifique a compris qu’il était possible
d’atteindre des performances inégalées grâce aux réseaux de neurones profonds,
déclenchant une explosion de recherches, d’applications commerciales et de
nouvelles architectures qui allaient bouleverser non seulement la vision par
ordinateur, mais aussi le traitement automatique du langage, la reconnaissance
vocale et bien d’autres domaines. Ce module vous guidera dans la découverte de
ce tournant historique et des concepts fondamentaux qui l’ont rendu possible.</p><h2 id=but-et-objectifs>But et objectifs<a class=anchor href=#but-et-objectifs>#</a></h2><p>Ce module vise à :</p><ul><li>Comprendre le fonctionnement des réseaux de neurones artificiels et leur rôle dans l’apprentissage profond.</li><li>Explorer les concepts fondamentaux : neurones, couches, fonctions d’activation, propagation avant et rétropropagation.</li><li>Découvrir l’impact historique d’AlexNet et des avancées récentes en vision par ordinateur et traitement du langage.</li><li>S’initier à l’interprétation visuelle et intuitive des réseaux de neurones grâce à des ressources pédagogiques (ex. : vidéo 3Blue1Brown).</li><li>Expérimenter concrètement avec des outils interactifs pour mieux saisir les mécanismes d’apprentissage.</li></ul><p><img src=https://cjauvin.github.io/inf1901-teluq/images/module3/ai-venn.png alt></p><h2 id=durée>Durée<a class=anchor href=#dur%c3%a9e>#</a></h2><p>Trois semaines ou 27 heures.</p><h2 id=évaluation>Évaluation<a class=anchor href=#%c3%a9valuation>#</a></h2><p>L’évaluation de ce module repose sur un travail noté où vous devrez manipuler un
réseau de neurones via une application interactive (Tensorflow Playground),
répondre à des questions d’interprétation et remettre un fichier PDF.</p></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div></div><div></div></div><div class="flex flex-wrap justify-between"><span><a href=https://cjauvin.github.io/inf1901-teluq/docs/module2/travail-not%C3%A9-2/ class="flex align-center"><img src=https://cjauvin.github.io/inf1901-teluq/icons/backward.svg class=book-icon alt=Backward>
<span>Travail noté 2</span>
</a></span><span><a href=https://cjauvin.github.io/inf1901-teluq/docs/module3/r%C3%A9seaux-de-neurones/ class="flex align-center"><span>Les réseaux de neurones</span>
<img src=https://cjauvin.github.io/inf1901-teluq/icons/forward.svg class=book-icon alt=Forward></a></span></div><div class=book-comments></div><script>(function(){document.querySelectorAll("pre:has(code)").forEach(e=>{e.addEventListener("click",e.focus),e.addEventListener("copy",function(t){if(t.preventDefault(),navigator.clipboard){const t=window.getSelection().toString()||e.textContent;navigator.clipboard.writeText(t)}})})})()</script></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#but-et-objectifs>But et objectifs</a></li><li><a href=#durée>Durée</a></li><li><a href=#évaluation>Évaluation</a></li></ul></nav></div></aside></main></body></html>