<!doctype html><html lang=fr dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="L’algorithme d’apprentissage le plus simple (les $k$ plus proches voisins)# Commençons par étudier le problème le plus fondamental et représentatif de l’apprentissage automatique, celui de la classification d’objets. Par exemple, pouvoir dire si l’image sur cette photo est celle d’un chien, ou d’un chat. Si on considère de nouveau notre scénario initial, on peut synthétiser la structure du problème de la manière suivante :
Nous avons à priori une série d’objets existants (des données) Chaque objet a des caractéristiques qui le décrivent, ainsi qu’une classe (on parle parfois aussi d’une étiquette, ou label en anglais) Il est possible de mesurer la distance qui sépare deux objets (en se basant sur les caractéristiques); cette distance peut être vue aussi comme une mesure de similarité, à quel point les deux objets sont semblables Pour un nouvel objet qu’on nous donne, dont on connaît les caractéristiques mais pas la classe (parce que personne nous l’a donnée), nous aimerions la prédire (la classe) L’algorithme le plus simple pour faire cela est assurément celui des $k$ plus proches voisins ($k$-NN, $k$ Nearest Neighbors). Pour chaque point que l’on désire classifier, il suffit de considérer ses $k$ plus proches voisins (proche, dans la plupart des contextes géométriques, veut dire en terme de la distance euclidienne) et de choisir la classe majoritaire. Notez que cet algorithme pourrait s’appeler également les $k$ voisins les plus semblables, car la distance peut également être interprétée en tant que mesure de similarité. L’applet interactive suivante illustre ce fonctionnement, à l’aide de points rouges et bleus en deux dimensions.
"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://cjauvin.github.io/inf1901-teluq/docs/module2/algo-le-plus-simple/"><meta property="og:site_name" content="INF1901 - Initiation à l'IA : concepts et réflexions"><meta property="og:title" content="L'algorithme le plus simple"><meta property="og:description" content="L’algorithme d’apprentissage le plus simple (les $k$ plus proches voisins)# Commençons par étudier le problème le plus fondamental et représentatif de l’apprentissage automatique, celui de la classification d’objets. Par exemple, pouvoir dire si l’image sur cette photo est celle d’un chien, ou d’un chat. Si on considère de nouveau notre scénario initial, on peut synthétiser la structure du problème de la manière suivante :
Nous avons à priori une série d’objets existants (des données) Chaque objet a des caractéristiques qui le décrivent, ainsi qu’une classe (on parle parfois aussi d’une étiquette, ou label en anglais) Il est possible de mesurer la distance qui sépare deux objets (en se basant sur les caractéristiques); cette distance peut être vue aussi comme une mesure de similarité, à quel point les deux objets sont semblables Pour un nouvel objet qu’on nous donne, dont on connaît les caractéristiques mais pas la classe (parce que personne nous l’a donnée), nous aimerions la prédire (la classe) L’algorithme le plus simple pour faire cela est assurément celui des $k$ plus proches voisins ($k$-NN, $k$ Nearest Neighbors). Pour chaque point que l’on désire classifier, il suffit de considérer ses $k$ plus proches voisins (proche, dans la plupart des contextes géométriques, veut dire en terme de la distance euclidienne) et de choisir la classe majoritaire. Notez que cet algorithme pourrait s’appeler également les $k$ voisins les plus semblables, car la distance peut également être interprétée en tant que mesure de similarité. L’applet interactive suivante illustre ce fonctionnement, à l’aide de points rouges et bleus en deux dimensions."><meta property="og:locale" content="fr"><meta property="og:type" content="article"><meta property="article:section" content="docs"><meta itemprop=name content="L'algorithme le plus simple"><meta itemprop=description content="L’algorithme d’apprentissage le plus simple (les $k$ plus proches voisins)# Commençons par étudier le problème le plus fondamental et représentatif de l’apprentissage automatique, celui de la classification d’objets. Par exemple, pouvoir dire si l’image sur cette photo est celle d’un chien, ou d’un chat. Si on considère de nouveau notre scénario initial, on peut synthétiser la structure du problème de la manière suivante :
Nous avons à priori une série d’objets existants (des données) Chaque objet a des caractéristiques qui le décrivent, ainsi qu’une classe (on parle parfois aussi d’une étiquette, ou label en anglais) Il est possible de mesurer la distance qui sépare deux objets (en se basant sur les caractéristiques); cette distance peut être vue aussi comme une mesure de similarité, à quel point les deux objets sont semblables Pour un nouvel objet qu’on nous donne, dont on connaît les caractéristiques mais pas la classe (parce que personne nous l’a donnée), nous aimerions la prédire (la classe) L’algorithme le plus simple pour faire cela est assurément celui des $k$ plus proches voisins ($k$-NN, $k$ Nearest Neighbors). Pour chaque point que l’on désire classifier, il suffit de considérer ses $k$ plus proches voisins (proche, dans la plupart des contextes géométriques, veut dire en terme de la distance euclidienne) et de choisir la classe majoritaire. Notez que cet algorithme pourrait s’appeler également les $k$ voisins les plus semblables, car la distance peut également être interprétée en tant que mesure de similarité. L’applet interactive suivante illustre ce fonctionnement, à l’aide de points rouges et bleus en deux dimensions."><meta itemprop=wordCount content="1271"><title>L'algorithme le plus simple | INF1901 - Initiation à l'IA : concepts et réflexions</title><link rel=icon href=https://cjauvin.github.io/inf1901-teluq/images/paperclip-logo.png><link rel=manifest href=https://cjauvin.github.io/inf1901-teluq/manifest.json><link rel=canonical href=https://cjauvin.github.io/inf1901-teluq/docs/module2/algo-le-plus-simple/><link rel=stylesheet href=https://cjauvin.github.io/inf1901-teluq/book.min.9f533add4c447a961b609e13ee02076c6d5baa3e0173d1a8f0c006e584b123e7.css integrity="sha256-n1M63UxEepYbYJ4T7gIHbG1bqj4Bc9Go8MAG5YSxI+c=" crossorigin=anonymous><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=stylesheet href=https://cjauvin.github.io/inf1901-teluq/css/applet.css></head><body dir=ltr class="book-kind-page book-type-docs"><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=https://cjauvin.github.io/inf1901-teluq/><img src=https://cjauvin.github.io/inf1901-teluq/images/paperclip-logo.png alt=Logo><span>INF1901 - Initiation à l'IA : concepts et réflexions</span></a></h2><ul><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/pr%C3%A9sentation/>Présentation du cours</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/philosophie/>Approche pédagogique du cours</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/professeurs/>Les professeurs</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/livres/>Les livres</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/travaux-not%C3%A9s/>Travaux notés</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/feuille-de-route/>Feuille de route</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/google-sheets/>Google Sheets</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/assistants-intelligents/>Usage de l'IA</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/changelog/>Évolution du cours (venez voir de temps en temps!)</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/discord/>Serveur Discord</a></li><li><input type=checkbox id=section-8907ef67e4ecdf3db52171242f091373 class=toggle>
<label for=section-8907ef67e4ecdf3db52171242f091373 class=flex><a href=https://cjauvin.github.io/inf1901-teluq/docs/module1/>Module 1 - Intelligence artificielle</a>
<img src=https://cjauvin.github.io/inf1901-teluq/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module1/activit%C3%A9s/>Activités</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module1/travail-not%C3%A9-1/>Travail noté 1</a></li></ul></li><li><input type=checkbox id=section-0e47afea11237f1612150a31e7123755 class=toggle checked>
<label for=section-0e47afea11237f1612150a31e7123755 class=flex><a href=https://cjauvin.github.io/inf1901-teluq/docs/module2/>Module 2 - Apprentissage automatique</a>
<img src=https://cjauvin.github.io/inf1901-teluq/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module2/sc%C3%A9nario-r%C3%A9el/>Un scénario pour se faire une idée</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module2/diff%C3%A9rence-avec-x/>AA versus X</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module2/les-donn%C3%A9es/>Que sont les données?</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module2/algo-le-plus-simple/ class=active>L'algorithme le plus simple</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module2/similarit%C3%A9/>Le concept de similarité</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module2/mod%C3%A8les/>Qu'est-ce qu'un modèle?</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module2/les-paradigmes/>Les paradigmes de l'AA</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module2/apprentissage-supervis%C3%A9/>Apprentissage supervisé</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module2/apprentissage-non-supervis%C3%A9/>Apprentissage non supervisé</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module2/travail-not%C3%A9-2/>Travail noté 2</a></li></ul></li><li><input type=checkbox id=section-9cc60359ac12343378e5fa944b3863f7 class=toggle>
<label for=section-9cc60359ac12343378e5fa944b3863f7 class=flex><a href=https://cjauvin.github.io/inf1901-teluq/docs/module3/>Module 3 - Réseaux de neurones et apprentissage profond</a>
<img src=https://cjauvin.github.io/inf1901-teluq/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module3/r%C3%A9seaux-de-neurones/>Les réseaux de neurones</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module3/02-3blue1brown/>3Blue1Brown</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module3/architectures-avanc%C3%A9es/>Architectures avancées</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module3/aa-adverse/>Apprentissage automatique adverse</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module3/travail-not%C3%A9-3/>Travail noté 3</a></li></ul></li><li><input type=checkbox id=section-d23b9c643b16319dcdaeed5376bcec9c class=toggle>
<label for=section-d23b9c643b16319dcdaeed5376bcec9c class=flex><a href=https://cjauvin.github.io/inf1901-teluq/docs/module4/>Module 4 - IA générative et grands modèles de langage</a>
<img src=https://cjauvin.github.io/inf1901-teluq/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module4/01-ia-g%C3%A9n%C3%A9rative/>IA générative</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module4/02-grands-mod%C3%A8les-de-langage/>Grands modèles de langage</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module4/03-3blue1brown/>3Blue1Brown</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module4/travail-not%C3%A9-4/>Travail noté 4</a></li></ul></li><li><input type=checkbox id=section-b90ca6a3362bb22be66d4f5e1f8ab16c class=toggle>
<label for=section-b90ca6a3362bb22be66d4f5e1f8ab16c class=flex><a href=https://cjauvin.github.io/inf1901-teluq/docs/module5/>Module 5 - Autour de l'IA</a>
<img src=https://cjauvin.github.io/inf1901-teluq/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module5/attitudes/>Attitudes à l'égard de l'IA</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module5/conversation/>Conversation synoptique autour de l'IA</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/module5/travail-not%C3%A9-5/>Travail noté 5</a></li></ul></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/ressources-additionnelles/>Ressources supplémentaires</a></li><li><a href=https://cjauvin.github.io/inf1901-teluq/docs/conclusion/>Conclusion</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class="book-header hidden"><div class="flex align-center justify-between"><label for=menu-control><img src=https://cjauvin.github.io/inf1901-teluq/icons/menu.svg class=book-icon alt=Menu></label><h3>L'algorithme le plus simple</h3><label for=toc-control><img src=https://cjauvin.github.io/inf1901-teluq/icons/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class=hidden><nav id=TableOfContents><ul><li><a href=#le-compromis-ou-dilemme-biais-variance>Le compromis, ou dilemme biais-variance</a></li><li><a href=#deux-types-derreur>Deux types d&rsquo;erreur</a></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=lalgorithme-dapprentissage-le-plus-simple-les-k-plus-proches-voisins>L&rsquo;algorithme d&rsquo;apprentissage le plus simple (les $k$ plus proches voisins)<a class=anchor href=#lalgorithme-dapprentissage-le-plus-simple-les-k-plus-proches-voisins>#</a></h1><p>Commençons par étudier le problème le plus fondamental et représentatif de
l&rsquo;apprentissage automatique, celui de la classification d&rsquo;objets. Par exemple,
pouvoir dire si l&rsquo;image sur cette photo est celle d&rsquo;un chien, ou d&rsquo;un chat. Si
on considère de nouveau notre <a href=https://cjauvin.github.io/inf1901-teluq/docs/module2/sc%C3%A9nario-r%C3%A9el/>scénario initial</a>, on peut synthétiser la structure du problème
de la manière suivante :</p><ul><li>Nous avons à priori une série d&rsquo;<strong>objets</strong> existants (des données)</li><li>Chaque objet a des <strong>caractéristiques</strong> qui le décrivent, ainsi qu&rsquo;une <strong>classe</strong> (on parle parfois aussi d&rsquo;une <em>étiquette</em>, ou label en anglais)</li><li>Il est possible de mesurer la <strong>distance</strong> qui sépare deux objets (en se basant sur les caractéristiques); cette distance peut être vue aussi comme une mesure de <em>similarité</em>, à quel point les deux objets sont semblables</li><li>Pour un <em>nouvel</em> objet qu&rsquo;on nous donne, dont on connaît les caractéristiques
mais <strong>pas</strong> la classe (parce que personne nous l&rsquo;a donnée), nous aimerions la
<em>prédire</em> (la classe)</li></ul><p>L&rsquo;algorithme le plus simple pour faire cela est assurément celui des $k$ plus
proches voisins ($k$-NN, $k$ Nearest Neighbors). Pour chaque point que l&rsquo;on
désire classifier, il suffit de considérer ses $k$ plus proches voisins
(<em>proche</em>, dans la plupart des contextes géométriques, veut dire en terme de la
<a href=https://fr.wikipedia.org/wiki/Distance_euclidienne>distance euclidienne</a>) et
de choisir la classe majoritaire. Notez que cet algorithme pourrait s&rsquo;appeler
également les $k$ <em>voisins les plus semblables</em>, car la distance peut également
être interprétée en tant que mesure de similarité. L&rsquo;applet interactive suivante
illustre ce fonctionnement, à l&rsquo;aide de points rouges et bleus en deux
dimensions.</p><p>Dans cet exemple :</p><ul><li>Les caractéristiques des points 2D sont leurs coordonnées $x$ et $y$</li><li>Leurs classes sont <code>rouge</code> ou <code>bleu</code></li><li>La distance entre les points est la <a href=https://fr.wikipedia.org/wiki/Distance_euclidienne>distance euclidienne</a></li><li>Les régions en bleu et rouge pâle correspondent à la classification de tout
nouveau point ajouté dans cette région (un point apparaît en cliquant)</li></ul><div class=applet-wrapper style=width:100%><iframe src=https://cjauvin.github.io/inf1901-teluq/html/applets/knn.html id=applet-1766868181039450793 class=applet-iframe width=100% loading=lazy data-iresize=true data-transform-scale=1.0></iframe></div><p>Pourquoi dit-on que cet algorithme est le plus simple? Parce que contrairement à d&rsquo;autres
algorithmes d&rsquo;apprentissage que nous verrons plus tard :</p><ul><li>Il y a un seul paramètres qui doit être &ldquo;appris&rdquo; : $k$, soit le nombre de voisins consultés;</li><li>Bien qu&rsquo;il y ait un jeu de données d’entraînement, il n&rsquo;y a pas de processus
d’entraînement, à proprement parler (contrairement à la plupart des
algorithmes que nous verrons par la suite): dès qu&rsquo;on a un ensemble de données
étiquetées, l&rsquo;algorithme est prêt à être utilisé; si on ajoute un nouveau
point, il est instantanément classé, en fonction de ses $k$ plus proches
voisins</li></ul><blockquote class="book-hint info"><p>Dans un sens, les données d’entraînement, accompagnées d&rsquo;un choix de valeur pour
$k$, constitue l&rsquo;algorithme, ou le modèle lui-même.</p></blockquote><h2 id=le-compromis-ou-dilemme-biais-variance>Le compromis, ou dilemme biais-variance<a class=anchor href=#le-compromis-ou-dilemme-biais-variance>#</a></h2><p>Nous allons maintenant introduire deux notions fondamentales en apprentissage
automatique, qui vont nous permettre d&rsquo;en comprendre l&rsquo;enjeu, ou la difficulté
principale.</p><p>Le <strong>biais</strong> d&rsquo;un modèle est l&rsquo;erreur qu&rsquo;il introduit avec ses hypothèses de
départ, ce qu&rsquo;il considère comme étant vrai à priori, avant même de commencer à
apprendre. Par exemple, si mon &ldquo;modèle&rdquo; du temps estimé pour me rendre au
travail est que &ldquo;ça prend toujours 20 minutes&rdquo;, alors dans certains cas, il sera en erreur, car il
n&rsquo;aura pas pris en considération le fait qu&rsquo;il pleut aujourd&rsquo;hui, ou que c&rsquo;est
le jour du Tour de l&rsquo;Ile à Montréal. C&rsquo;est un modèle beaucoup trop général, au point où
il est très peu utile, car trop flou.</p><p>La <strong>variance</strong> d&rsquo;un modèle est l&rsquo;erreur qui est introduite quand j&rsquo;essaie de
bâtir un modèle à partir de <em>ces</em> exemples particuliers (ces données
d’entraînement particulières, que j&rsquo;ai possiblement obtenu par hasard) plutôt
que <em>ceux-là</em>. Il s&rsquo;agit donc de l&rsquo;erreur qui correspond aux variations
naturelles, ou accidentelles, qu&rsquo;on observe dans la nature, ce qu&rsquo;on nomme
parfois aussi le <strong>bruit</strong>. Par exemple, si mon &ldquo;modèle&rdquo; du temps estimé pour me
rendre au travail est basé sur mes observations d&rsquo;une semaine particulière, donc :
&ldquo;20 minutes le lundi&rdquo;, &ldquo;30 minutes le mardi&rdquo;, et ainsi de suite, il est très
probable que ce modèle colle de trop près à la réalité, et qu&rsquo;il tente trop de
généraliser à partir de ce qui n&rsquo;est, au fond, que des fluctuations aléatoires
(le fait que ça m&rsquo;a pris 20 minutes pour me rendre au travail lundi passé est
assez peu corrélé avec le temps que ça me prendra le lundi suivant, et c&rsquo;est
probablement une erreur de trop vouloir généraliser). Ce modèle est trop
spécifique, pas assez général.</p><p>Nous pouvons donc analyser notre algorithme des plus proches voisins à la
lumière de ces notions : quand $k$ est petit, la variance du modèle est très
élevée, et les particularités individuelles des données (le fait que <em>ce</em> point
rouge soit exactement <em>ici</em>, plutôt que <em>là</em>) ont une grande importance. On
parle ici de <strong>sur-apprentissage</strong> (overfitting). Visuellement, on peut
constater ceci en considérant que la ligne de décision (l&rsquo;endroit où la zone
rouge pâle du fond devient bleue pâle, et qui marque le classement de tout
nouveau point éventuel) est complexe et fragmentée, car elle épouse presque
parfaitement les particularités de ce jeu de données particulier, afin d&rsquo;éviter
toute erreur (et d&rsquo;ailleurs on remarque aussi que cette configuration change
complètement, dès qu&rsquo;on régénère de nouvelles données aléatoires).</p><p><img src=https://cjauvin.github.io/inf1901-teluq/images/module2/knn-small-k.png alt></p><p>À l&rsquo;inverse, quand $k$ est très grand, c&rsquo;est le biais qui devient très élevé :
le modèle prend en considération un très grand nombre de facteurs (c-à-d de
points) pour prendre une décision, et probablement qu&rsquo;il s&rsquo;agit d&rsquo;une
généralisation excessive. On parle alors de <strong>sous-apprentissage</strong>
(underfitting). La ligne de décision entre les zones rouges et bleues devient
alors plus linéaire, et moins changeante, car elle représente une décision
moyenne, plus tolérante aux erreurs potentielles avec les données
d’entraînement. Le modèle aurait probablement avantage, dans ce cas, à
considérer les données de manière un peu plus spécifique.</p><p><img src=https://cjauvin.github.io/inf1901-teluq/images/module2/knn-big-k.png alt></p><p>On considère en général que ces deux notions sont l&rsquo;inverse, l&rsquo;une de l&rsquo;autre :
quand le biais d&rsquo;un modèle augmente, sa variance diminue, et vice versa.
L&rsquo;apprentissage automatique constitue donc l&rsquo;art de trouver un bon compromis
entre ces deux extrêmes.</p><p><img src=https://cjauvin.github.io/inf1901-teluq/images/module2/bias-vs-variance.png alt></p><h2 id=deux-types-derreur>Deux types d&rsquo;erreur<a class=anchor href=#deux-types-derreur>#</a></h2><p>Quand on joue avec l&rsquo;applet interactive ci-haut, et qu&rsquo;on choisit $k=1$, on
remarque que l&rsquo;erreur est zéro. Ceci est dû au fait que par définition, un point
est lui-même inclus dans ses $k$ voisins, donc quand le point est seul, il ne
peut y avoir aucune erreur. En augmentant $k$, le nombre d&rsquo;erreurs augmente
généralement. On pourrait imaginer qu&rsquo;un algorithme qui ne commet aucune erreur
est désirable, mais dans le cas de $k$-NN, ce résultat en apparence
impressionnant est tout simplement trivial, car il est obtenu par définition.
Comme cette erreur est calculée sur le jeu de données correspondant à
l’entraînement, on parle alors d&rsquo;<strong>erreur d’entraînement</strong>. En apprentissage
automatique, il est souvent possible, avec différents algorithmes, de faire en
sorte que cette erreur soit très proche de zéro. Pourtant, l&rsquo;erreur qui nous
intéresse réellement est celle calculée à partir d&rsquo;un autre jeu de données, qui
n&rsquo;a <em>pas</em> participé à l’entraînement du modèle : on parle alors de l&rsquo;<strong>erreur de
test</strong>. Contrairement à l&rsquo;erreur d’entraînement, qui est minimisée dans le cas
des $k$ plus proches voisins quand $k=1$, l&rsquo;erreur de test aura tendance à être
minimisée à la valeur de $k$ qui correspond au meilleur compromis
biais-variance, comme l&rsquo;illustre ce diagramme :</p><p><img src=https://cjauvin.github.io/inf1901-teluq/images/module2/bias-vs-variance-with-errors.png alt></p><p>Nous verrons que ce principe est très général en apprentissage automatique, et
qu&rsquo;il permet de guider la recherche du &ldquo;meilleur&rdquo; modèle, celui dont les
capacités de généralisation sont les plus grandes, ce qui constitue un des buts
fondamentaux de l&rsquo;intelligence artificielle.</p><blockquote class="book-hint warning"><p>Des résultats récents en apprentissage profond ont toutefois apporté de grandes
surprises, car il semble que ce principe, qui était considéré comme étant
immuable et indiscutable depuis des décennies, peut être, dans certaines
circonstances, <a href=https://en.wikipedia.org/wiki/Double_descent>remis en
question</a> !</p></blockquote></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div></div><div></div></div><div class="flex flex-wrap justify-between"><span><a href=https://cjauvin.github.io/inf1901-teluq/docs/module2/les-donn%C3%A9es/ class="flex align-center book-icon"><img src=https://cjauvin.github.io/inf1901-teluq/svg/backward.svg alt=Previous title="Que sont les données?">
<span>Que sont les données?</span>
</a></span><span><a href=https://cjauvin.github.io/inf1901-teluq/docs/module2/similarit%C3%A9/ class="flex align-center book-icon"><span>Le concept de similarité</span>
<img src=https://cjauvin.github.io/inf1901-teluq/svg/forward.svg alt=Next title="Le concept de similarité"></a></span></div><div class=book-comments></div><script>(function(){document.querySelectorAll("pre:has(code)").forEach(e=>{e.addEventListener("click",e.focus),e.addEventListener("copy",function(t){if(t.preventDefault(),navigator.clipboard){const t=window.getSelection().toString()||e.textContent;navigator.clipboard.writeText(t)}})})})()</script></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#le-compromis-ou-dilemme-biais-variance>Le compromis, ou dilemme biais-variance</a></li><li><a href=#deux-types-derreur>Deux types d&rsquo;erreur</a></li></ul></nav></div></aside></main><script src=https://cdn.jsdelivr.net/npm/iframe-resizer/js/iframeResizer.min.js defer></script><script>document.addEventListener("DOMContentLoaded",()=>{const e=document.querySelectorAll('iframe[data-iresize="true"]');if(!e.length||typeof iFrameResize!="function")return;const t={license:"GPLv3",checkOrigin:!1,onResized:function({iframe:e,height:t}){const s=e?.parentElement;if(!s)return;const o=e.getAttribute("data-transform-scale")||"1";let n=parseFloat(o);(!Number.isFinite(n)||n<=0)&&(n=1),s.style.height=t*n+"px"}};e.forEach(e=>{const n={...t},s=e.getAttribute("data-check-origin");s==="true"&&(n.checkOrigin=!0),s==="false"&&(n.checkOrigin=!1),e.getAttribute("data-log")==="true"&&(n.log=!0);const o=e.getAttribute("data-transform-scale")||"1";e.style.transform=`scale(${o})`,e.iFrameResizer||iframeResize(n,e)})})</script></body></html>